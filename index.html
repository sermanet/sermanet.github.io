<!doctype html>
<head>
  <title>Pierre Sermanet Homepage</title>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width,initial-scale=1.0" />
  <meta http-equiv="X-UA-Compatible" content="ie=edge" />
  <link href='https://fonts.googleapis.com/css?family=Roboto:300' rel='stylesheet' type='text/css'>
  <meta name="theme-color" content="#1a4067" />
  <!-- SEO -->
  <meta property="og:title" content="Pierre Sermanet Homepage" />
  <meta property="og:type" content="article" />
  <meta property="og:description" content="" />
  <meta property="og:image" content="https://sermanet.github.io/assets/img/lmp_logo_rect.png" />
  <meta property="og:url" content="https://sermanet.github.io/" />
  <!-- Twitter Card data -->
  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Pierre Sermanet Homepage" />
  <meta name="twitter:description" content="" />
  <meta property="og:site_name" content="" />
  <meta name="twitter:image" content="https://sermanet.github.io/assets/img/lmp_logo_square.png" />

  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.5.1/katex.min.css">
  <link rel="stylesheet" href="/style.css">
</head>
<body>

<script src="lib/jquery-1.12.4.min.js"></script>
<!--<script src="lib/mobile-detect.min.js"></script>-->
<script src="lib/template.v1.js"></script>

<!-- <div class="cover"> -->
<!--   <h1 class="unselectable">Learning Latent Plans<br>from Play</h1> -->
<!--   <video src="assets/mp4/8tasks1920x540.mp4" autoplay loop playsinline muted></video> -->
<!--   <div class="hint unselectable">scroll down</div> -->
<!-- </div> -->

<dt-article id="dtbody">
<dt-byline class="l-page transparent"></dt-byline>
<h1>Pierre Sermanet</h1>
<h2>Research Scientist @ <a class="project-link" href="https://deepmind.google/" target="_blank">Google DeepMind</a><br>
  Machine Learning for Language / Vision / Actions / Safety
</h2>
<dt-byline class="l-page" id="authors_section">
<div class="byline">
  <div class="authors">
    <div class="author"><a class="project-link" href="https://scholar.google.com/citations?user=0nPi5YYAAAAJ" target="_blank">Scholar</a></div>
    <div class="author"><a class="project-link" href="https://twitter.com/psermanet" target="_blank">Twitter</a></div>
    <div class="author"><a class="project-link" href="https://www.youtube.com/user/nyulagr" target="_blank">Youtube</a></div>
    <div class="author"><a class="project-link" href="https://github.com/sermanet" target="_blank">GitHub</a></div>
    <div class="author"><a class="project-link" href="https://www.kaggle.com/pierresermanet" target="_blank">Kaggle</a></div>
    <div class="author"><a class="project-link" href="https://www.linkedin.com/in/sermanet/" target="_blank">LinkedIn</a></div>
    <div class="author"><a class="project-link" href="">Email: first dot last at gmail</a></div>
    <br>
    <br>

<h3>Selected highlights</h3>
<ul class="highlights-list">
  <li><strong>2025</strong> — ~100k <a href="https://scholar.google.com/citations?user=0nPi5YYAAAAJ">citations</a> across machine learning, computer vision, and robotics + <a href="https://sermanet.github.io/#asimov">Laws of robotics</a> generated from images and <a href="https://sermanet.github.io/#scifi">sci-fi</a></li>
  <li><strong>2023</strong> — Open-sourced large-scale <a href="https://sermanet.github.io/#robovqa">VQA benchmark</a> for robotics</li>
  <li><strong>2019</strong> — Co-developed the first end-to-end <a href="https://sermanet.github.io/#language-play"><strong>Vision-Language-Action</strong></a> <strong>(VLA) models</strong> for robotic manipulation learned from unstructured videos with Corey Lynch (now Head of AI at Figure)</li>
  <li><strong>2017</strong> — Fully <a href="https://sermanet.github.io/imitate/">self-supervised imitation from video</a> -> robot policy <strong>learned without any labels</strong>!</li>
  <li><strong>2016</strong> — Founding member of Google Brain Robotics team + <a href="https://sermanet.github.io/#perceptual">unsupervised perceptual rewards</a></li>
  <li><strong>2014</strong> — Winner of <a href="http://www.kaggle.com/c/dogs-vs-cats/leaderboard">Dogs vs Cats</a> and <a href="https://image-net.org/challenges/LSVRC/2014/">2014 ImageNet</a> competitions</li>
  <li><strong>2013</strong> — PhD with Yann LeCun, <strong>open-sourced one of the first deep vision models</strong>; winner of ImageNet competition</li>
  <li><strong>2009</strong> — Designed and released an <strong>early open-source deep learning framework</strong> (<a href="https://sermanet.github.io/#eblearn">EBLearn</a>)</li>
  <li><strong>2006</strong> — Proposed a <a href="https://sermanet.github.io/#fast-slow"><strong>fast & slow thinking</strong></a> <strong>architecture</strong> for robotics (system 1 / system 2)</li>
  <li><strong>2005</strong> — <a href="https://sermanet.github.io/#lagr-deep-learning">Early work</a> on <strong>deep learning for robotics</strong> in DARPA’s LAGR program</li>
</ul>
</div>
      
    <!-- I am a Research Scientist at Google DeepMind, working at the intersection of language, vision and actions with applications to robotics and embodied AI.
    My research interests span from multimodal learning, computer vision, self-supervised learning, learning from play, scalable data acquisition
    to embodied reasoning and AGI.
    <br>
    <br>
    I obtained a PhD at New York University under the supervision of Yann LeCun in 2013 during which I published on various topics in computer vision (object detection, the “Overfeat” algorithm, pedestrian detection, visual attention, etc) and machine learning, e.g on unsupervised feature learning.
    <br>
    <br>
    Research: After joining Google Brain in 2014 (now Google DeepMind),
    I continued working on self-supervised learning, computer vision, robotics, visual language models (VLM) and embodied AI.
    Because data is expensive to acquire for robotics and arguably its biggest bottleneck, I have developed self-supervision
    and contrastive learning techniques to learn from cheap, abundant and embodiment-agnostic (robot, human) unstructured video data
    (<a class="link" href="https://sermanet.github.io/#tcn" target="_blank">Time-Contrastive networks</a>, 
    <a class="link" href="https://sermanet.github.io/#perceptual" target="_blank">Unsupervised Perceptual Rewards</a>),
    as well as natural and information-rich multimodal data acquisition techniques for embodied AI 
    (<a class="link" href="https://sermanet.github.io/#play" target="_blank">Learning from Play</a>, 
    <a class="link" href="https://sermanet.github.io/#robovqa" target="_blank">RoboVQA</a>)
    and training integrated Vision-Language-Action models 
    (<a class="link" href="https://sermanet.github.io/#language-play" target="_blank">Grounding Language in Play</a>,
    <a class="link" href="https://sermanet.github.io/#rt2" target="_blank">Robotics Transformers 2</a>).
    <br>
    <br>    
    Competitions: <a class="link" href="https://sermanet.github.io/#overfeat" target="_blank">OverFeat</a> ranked 1st place in both the
    <a class="link" href="https://image-net.org/challenges/LSVRC/2013/results.php" target="_blank">2013 ImageNet</a> object localization challenge 
    and the 2014 <a class="link" href="http://www.kaggle.com/c/dogs-vs-cats/leaderboard" target="_blank">Dogs vs Cats</a> Kaggle competition 
    (<a class="link" href="https://www.kaggle.com/pierresermanet" target="_blank">profile</a>),
    <a class="link" href="https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Szegedy_Going_Deeper_With_2015_CVPR_paper.pdf" target="_blank">GoogLeNet</a> ranked 1st in the 
    <a class="link" href="https://image-net.org/challenges/LSVRC/2014/" target="_blank">2014 ImageNet</a> challenge for both Image Classification and Object detection challenges and 2nd in the Object Localization challenge. In 2011, the deep learning framework I developed “EBLearn” ranked 2nd in the 
    <a class="link" href="https://benchmark.ini.rub.de/gtsrb_results.html" target="_blank">traffic sign</a> recognition challenge.
  </div> -->
  <!-- <div class="date"> -->
  <!--   <div class="month">March 5</div> -->
  <!--   <div class="year">2019</div> -->
  <!-- </div> -->
  <!-- <div class="date"> -->
  <!--   <div class="month">Download</div> -->
  <!--   <div class="year" style="color: #0000FF;"><a href="https://arxiv.org/" target="_blank">PDF</a></div> -->
  <!-- </div> -->
</div>
</dt-byline>
<!-- </dt-byline> -->
<dt-byline class="l-page" id="authors_section">
  Selected Projects and Publications (<a class="link" href="https://scholar.google.com/citations?user=0nPi5YYAAAAJ" target="_blank">Full List</a>)
<div class="byline">
<table valign="top">

  <tr>    
    <td class="project-fig"><div class="figure">
      <img class='project-img' src='assets/scifi/scifi_teaser.png'>
    </td>
    <td class="project-cell">
      <div class="project-title" id="robovqa">SciFi-Benchmark: How Would AI-Powered Robots Behave in Science Fiction Literature?</div>
      <dt-byline><div class="byline">
      <a class="project-link" href=https://scifi-benchmark.github.io/ target="_blank">Project Page</a>
      <a class="project-link" href=https://arxiv.org/abs/2503.10706 target="_blank">Paper</a>
      Data: 
      <a class="project-link" href=https://www.tensorflow.org/datasets/catalog/asimov_dilemmas_scifi_val target="_blank">val</a>|
      <a class="project-link" href=https://www.tensorflow.org/datasets/catalog/asimov_dilemmas_scifi_train target="_blank">train</a>
      <a class="project-link" href=https://github.com/scifi-benchmark/safety_cards target="_blank">Safety Cards</a>
      <a class="project-link" href=https://github.com/sermanet/sermanet.github.io/blob/master/assets/bib/Sermanet2025SciFi.bib target="_blank">BibTex</a>
      <a class="project-link" href=https://x.com/psermanet/status/1902014690164903955 target="_blank">Twitter</a>
      </div></div><br>
      Pierre Sermanet, Anirudha Majumdar, Vikas Sindhwani
      <br><br>
      Q: How would AI-powered robots behave if dropped into Science Fiction literature?<br>
      A: 95.8% aligned with humans (Sci-Fi decisions are only 21.2% aligned).
      <br>
      <br>
      Q: Can we generate useful robot constitutions 📜 from Sci-Fi?<br>
      A: Sci-Fi inspired constitutions yield some of the strongest alignment on realistic safety benchmarks.
    </td>
  </tr>

  <tr>    
    <td class="project-fig"><div class="figure">
      <img class='project-img' src='assets/asimov/asimov_generation.png'>
      <!-- <img class='project-img' src='assets/asimov/robot_constitutions.png'> -->
    </td>
    <td class="project-cell">
      <div class="project-title" id="robovqa">Generating Robot Constitutions<br>& Benchmarks for Semantic Safety</div>
      <dt-byline><div class="byline">
      <a class="project-link" href=https://asimov-benchmark.github.io/ target="_blank">Project Page</a>
      <a class="project-link" href=https://arxiv.org/pdf/2503.08663 target="_blank">Paper</a>
      <a class="project-link" href=https://github.com/asimov-benchmark/code/blob/main/ASIMOV_Datasets.ipynb target="_blank">Data</a>
      <a class="project-link" href=https://github.com/sermanet/sermanet.github.io/blob/master/assets/bib/Sermanet2025SciFi.bib target="_blank">BibTex</a>
      <a class="project-link" href=https://x.com/psermanet/status/1900203328526766429 target="_blank">Twitter</a>
      </div></div><br>
      Pierre Sermanet, Anirudha Majumdar, Alex Irpan, Dmitry Kalashnikov, Vikas Sindhwani
      <br><br>
      Q: How can we ensure robots behave properly at scale?<br>A: Robot constitutions 📜!<br>
      Q: How do we verify behavior in undesirable situations at scale?<br>A: Generation!<br>
      <br>
      We release the ASIMOV Benchmark for Semantic Safety of robots. We generate difficult-to-capture undesirable situations using image generation.<br>
      We also generate robot constitutions straight from images.

    </td>
  </tr>
  
  <tr>    
    <td class="project-fig"><div class="figure">
      <video class="b-lazy" data-src="assets/robovqa/meet_human.mp4"
        type="video/mp4" autoplay muted playsinline loop style="display: block; width:100%;"></video></div>
      <br>
      <video class="b-lazy" data-src="assets/robovqa/meet_robot.mp4"
        type="video/mp4" autoplay muted playsinline loop style="display: block; width:100%;"></video></div>
    </td>
    <td class="project-cell">
      <div class="project-title" id="robovqa">RoboVQA: Multimodal Long-Horizon Reasoning for Robotics</div>
      <dt-byline><div class="byline">
      <a class="project-link" href=https://robovqa.github.io/>Project Page</a>
      <a class="project-link" href=https://arxiv.org/pdf/2311.00899.pdf>Paper</a>
      <a class="project-link" href=https://robovqa.github.io/>Data</a>
      <a class="project-link" href=https://github.com/sermanet/sermanet.github.io/blob/master/assets/bib/Sermanet2024RoboVQA.bib>BibTex</a>
      <a class="project-link" href=https://twitter.com/psermanet/status/1722610200430190784>Twitter</a>
      </div></div><br>
      Pierre Sermanet, Tianli Ding, Jeffrey Zhao, Fei Xia, Yuan Cao <a class="link" href="https://arxiv.org/pdf/2311.00899.pdf">et al.</a> @ <a href="https://2024.ieee-icra.org/">ICRA 2024</a>, 2023 <a class="link" href="https://sites.google.com/view/langrob-corl23/home">CoRL</a> and <a class="link" href="https://sites.google.com/corp/view/fmdm-neurips23/?pli=1">NeurIPS</a> workshops
      <br><br>
      Need help in the real world? RoboVQA can guide robots and humans through long-horizon tasks on a phone via Google Meet.
      <br><br>We propose scalable real-world data acquisition and augmentation strategies and release a <a class="link"  href="http://robovqa.github.io">dataset</a> of 800k (video, question/answer) with robots & humans doing various long-horizon tasks.
      <br><br>We train a small video model (380M) that outperforms large state of the art zero-shot models by ~2x and demonstrate that scalable strategies for acquiring new data remains criticial.
      <br><br>We use a speech intervention mechanism that automatically quantifies progress (intervention rate), provides corrections to retrain on, and allows performing tasks to completion in supervised real-world deployment.
    </td>
  </tr>

  <!-- 2-column project block -->
  <tr>    
    <td class="project-cell" colspan="2">
      <div class="project-title" id="rtx">Open X-Embodiment: Robotic Learning Datasets and RT-X Models</div>
      <dt-byline><div class="byline">
      <a class="project-link" href=https://robotics-transformer-x.github.io/>Project Page</a>
      <a class="project-link" href=https://arxiv.org/abs/2310.08864>Paper</a>
      <a class="project-link" href=https://robotics-transformer-x.github.io/citation.txt>BibTex</a>
      </div></div><br>A. Padalkar, A. Pooley, A. Jain, A. Bewley, A. Herzog, A. Irpan, A. Khazatsky, A. Rai, A. Singh, A. Brohan, A. Raffin, A. Wahid, B. Burgess-Limerick, B. Kim, B. Schölkopf, B. Ichter, C. Lu, C. Xu, C. Finn, C. Xu, C. Chi, C. Huang, C. Chan, C. Pan, C. Fu, C. Devin, D. Driess, D. Pathak, D. Shah, D. Büchler, D. Kalashnikov, D. Sadigh, E. Johns, F. Ceola, F. Xia, F. Stulp, G. Zhou, G. S. Sukhatme, G. Salhotra, G. Yan, G. Schiavi, G. Kahn, H. Su, H. Fang, H. Shi, H. Ben Amor, H. I Christensen, H. Furuta, H. Walke, H. Fang, I. Mordatch, I. Radosavovic, I. Leal, J. Liang, J. Abou-Chakra, J. Kim, J. Peters, J. Schneider, J. Hsu, J. Bohg, J. Bingham, J. Wu, J. Wu, J. Luo, J. Gu, J. Tan, J. Oh, J. Malik, J. Tompson, J. Yang, J. J. Lim, J. Silvério, J. Han, K. Rao, K. Pertsch, K. Hausman, K. Go, K. Gopalakrishnan, K. Goldberg, K. Byrne, K. Oslund, K. Kawaharazuka, K. Zhang, K. Rana, K. Srinivasan, L. Y. Chen, L. Pinto, L. Tan, L. Ott, L. Lee, M. Tomizuka, M. Du, M. Ahn, M. Zhang, M. Ding, M. K. Srirama, M. Sharma, M. J. Kim, N. Kanazawa, N. Hansen, N. Heess, N. J. Joshi, N. Suenderhauf, N. Di Palo, N. M. M. Shafiullah, O. Mees, O. Kroemer, P. R. Sanketi, P. Wohlhart, P. Xu, P. Sermanet, P. Sundaresan, Q. Vuong, R. Rafailov, R. Tian, R. Doshi, R. Martín-Martín, R. Mendonca, R. Shah, R. Hoque, R. Julian, S. Bustamante, S. Kirmani, S. Levine, S. Moore, S. Bahl, S. Dass, S. Sonawani, S. Song, S. Xu, S. Haldar, S. Adebola, S. Guist, S. Nasiriany, S. Schaal, S. Welker, S. Tian, S. Dasari, S. Belkhale, T. Osa, T. Harada, T. Matsushima, T. Xiao, T. Yu, T. Ding, T. Davchev, T. Z. Zhao, T. Armstrong, T. Darrell, V. Jain, V. Vanhoucke, W. Zhan, W. Zhou, W. Burgard, X. Chen, X. Wang, X. Zhu, X. Li, Y. Lu, Y. Chebotar, Y. Zhou, Y. Zhu, Y. Xu, Y. Wang, Y. Bisk, Y. Cho, Y. Lee, Y. Cui, Y. Wu, Y. Tang, Y. Zhu, Y. Li, Y. Iwasawa, Y. Matsuo, Z. Xu, Z. J. Cui
        <br>@ CoRL 2023 
    </td>
  </tr>
  <!-- 2-column project block -->
  <tr>    
    <td class="project-fig"><div class="figure">
      <img class='project-img' src='assets/rt2/RT2_fig2.png'>
      <img class='project-img' src='assets/rt2/RT2_CoT5.png'>
    </div></td>    
    <td class="project-cell" colspan="2">
      <div class="project-title" id="rt2">RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control</div>
      <dt-byline><div class="byline">
      <a class="project-link" href=https://robotics-transformer2.github.io/>Project Page</a>
      <a class="project-link" href=https://arxiv.org/pdf/2307.15818.pdf>Paper</a>
      <a class="project-link" href=https://karolhausman.github.io/bibtex/rt2-2023arxiv.bib>BibTex</a>
      </div></div><br>  
        A. Brohan, N. Brown, J. Carbajal, Y. Chebotar, X. Chen, K. Choromanski, T. Ding, D. Driess, A. Dubey, C. Finn, P. Florence, C. Fu, M. Gonzalez Arenas, K. Gopalakrishnan, K. Han, K. Hausman, A. Herzog, J. Hsu, B. Ichter, A. Irpan, N. Joshi, R. Julian, D. Kalashnikov, Y. Kuang, I. Leal, L. Lee, T. E. Lee, S. Levine, Y. Lu, H. Michalewski, I. Mordatch, K. Pertsch, K. Rao, K. Reymann, M. Ryoo, G. Salazar, P. Sanketi, P. Sermanet, J. Singh, A. Singh, R. Soricut, H. Tran, V. Vanhoucke, Q. Vuong, A. Wahid, S. Welker, P. Wohlhart, J. Wu, F. Xia, T. Xiao, P. Xu, S. Xu, T. Yu, B. Zitkovich
        <br>@ CoRL 2023 
    </td>
  </tr>
  <!-- 2-column project block -->
  <tr>    
    <td class="project-cell" colspan="2">
      <div class="project-title" id="palme">PaLM-E: An Embodied Multimodal Language Model</div>
      <dt-byline><div class="byline">
      <a class="project-link" href=https://palm-e.github.io/>Project Page</a>
      <a class="project-link" href=https://arxiv.org/abs/2303.03378>Paper</a>
      <a class="project-link" href=https://github.com/sermanet/sermanet.github.io/blob/master/assets/bib/driess2023palme.bib>BibTex</a>
      </div></div><br>  
        Danny Driess, Fei Xia, Mehdi S. M. Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, Wenlong Huang, Yevgen Chebotar, Pierre Sermanet, Daniel Duckworth, Sergey Levine, Vincent Vanhoucke, Karol Hausman, Marc Toussaint, Klaus Greff, Andy Zeng, Igor Mordatch, Pete Florence
        <br>@ ICML 2023
    </td>
  </tr>
  <!-- 2-column project block -->
  <tr>    
    <td class="project-cell" colspan="2">
      <div class="project-title" id="innermonologue">Inner Monologue: Embodied Reasoning through Planning with Language Models</div>
      <dt-byline><div class="byline">
      <a class="project-link" href=https://innermonologue.github.io/>Project Page</a>
      <a class="project-link" href=https://arxiv.org/abs/2207.05608>Paper</a>
      <a class="project-link" href=https://www.youtube.com/watch?v=0sJjdxn5kcI>Video</a>
      <a class="project-link" href=https://karolhausman.github.io/bibtex/huang22inner.bib>BibTex</a>
      </div></div><br>  
        Wenlong Huang*, Fei Xia*, Ted Xiao*, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, Pierre Sermanet, Noah Brown, Tomas Jackson, Linda Luu, Sergey Levine, Karol Hausman, Brian Ichter. 
        <br>@ CoRL 2022
    </td>
  </tr>
  <!-- 2-column project block -->
  <tr>    
    <td class="project-cell" colspan="2">
      <div class="project-title" id="saycan">(SayCan) Do As I Can, Not As I Say: Grounding Language in Robotic Affordances</div>
      <dt-byline><div class="byline">
      <a class="project-link" href=https://say-can.github.io/>Project Page</a>
      <a class="project-link" href=https://say-can.github.io/assets/palm_saycan.pdf>Paper</a>
      <a class="project-link" href=https://www.youtube.com/watch?v=ysFav0b472w>Video</a>
      <a class="project-link" href=https://karolhausman.github.io/bibtex/saycan22arxiv.bib>BibTex</a>
      </div></div><br>
        M. Ahn, A. Brohan, N. Brown, Y. Chebotar, O. Cortes, B. David, C. Fu, Ch. Finn, K. Gopalakrishnan, K. Hausman, A. Herzog, D. Ho, J. Hsu, J. Ibarz, B. Ichter, A. Irpan, E. Jang, R. Jauregui Ruano, K. Jeffrey, S. Jesmonth, N. J Joshi, R. Julian, D. Kalashnikov, Y. Kuang, K.-H. Lee, S. Levine, Y. Lu, L. Luu, C. Parada, P. Pastor, J. Quiambao, K. Rao, J. Rettinghouse, D. Reyes, P. Sermanet, N. Sievers, Cl. Tan, A. Toshev, V. Vanhoucke, F. Xia (co-corresponding author), T. Xiao, P. Xu, S. Xu, M. Yan, A. Zeng (alphebatically listed)
        <br>@ CoRL 2022
    </td>
  </tr>  
  <!-- project block -->
  <tr>    
    <td class="project-fig"><div class="figure"><video class="b-lazy" data-src="assets/langlfp/playlang_20200326-193259_13tasks_bt300k.mp4"
        type="video/mp4" autoplay muted playsinline loop style="display: block; width:100%;"></video></div></td>
    <td class="project-cell">
      <div class="project-title" id="language-play">Grounding Language in Play</div>
      <dt-byline><div class="byline">
      <a class="project-link" href=https://sermanet.github.io/language-play>Project Page</a>
      <a class="project-link" href=https://arxiv.org/pdf/2005.07648.pdf>Paper</a>
      <a class="project-link" href=https://github.com/sermanet/sermanet.github.io/blob/master/assets/bib/Lynch2020Language.bib>BibTex</a>
      </div></div><br>
      Corey Lynch and Pierre Sermanet @ <a href="http://www.roboticsproceedings.org/rss17/p047.pdf">RSS 2021</a>
      <br><br>
      We present a simple and scalable approach for controlling robots with natural language: play through teleoperation, then answer “how do I go from start to finish?” for random episodes. We can then type in commands in real time.
      <br><br>By hooking up our English-trained model with a pre-trained language embedding trained on lots of text and different languages, it not only improves control but also allows commanding the model in 16 languages.
      <br><br>Combining natural language with play provides a breadth of skills while having no tasks determined in advance. This yields flexible specification of tasks, for example we can compose tasks on the fly: “pick up the object”, then “put the object in the trash”.
    </td>
  </tr>
  <!-- project block -->
  <tr>
    <td class="project-fig"><div class="figure"><video class="b-lazy" data-src="assets/lmp/mp4/8tasks_cropped_640x180.mp4"
        type="video/mp4" autoplay muted playsinline loop style="display: block; width:100%;"></video></div></td>
    <td class="project-cell">
      <div class="project-title" id="play">Learning Latent Plans from Play</div>
      <dt-byline><div class="byline">
      <a class="project-link" href=https://sermanet.github.io/learning-from-play>Project Page</a>
      <a class="project-link" href=https://arxiv.org/pdf/1903.01973.pdf>Paper</a>
      <a class="project-link" href=https://github.com/sermanet/sermanet.github.io/blob/master/assets/bib/Lynch2019Play.bib>BibTex</a>
      </div></div><br>
      Corey Lynch, Mohi Khansari, Ted Xiao, Vikash Kumar, Jonathan Tompson, Sergey Levine, Pierre Sermanet @ CoRL 2019
      <br><br>
      How to scale-up multi-task learning?<br>
      Self-supervise plan representations from lots of cheap unlabeled play data (no RL was used).
    </td>
  </tr>
  <!-- project block -->
  <tr>
    <td class="project-fig"><div class="figure">
      <video class="b-lazy" data-src="assets/mftcn/cheetah.mp4"
        type="video/mp4" autoplay muted playsinline loop style="display: block; width: 100%;"></video>
      <img class='project-img' src='assets/mftcn/model.png'>
    </div></td>
    <td class="project-cell">
      <div class="project-title">Self-Supervised Actionable Representations</div>
      <dt-byline><div class="byline">
      <a class="project-link" href=https://sites.google.com/view/actionablerepresentations/>Project Page</a>
      <a class="project-link" href=https://arxiv.org/abs/1808.00928>Paper</a>
      <a class="project-link" href=https://github.com/sermanet/sermanet.github.io/blob/master/assets/bib/Dwibedi2018Learning.bib>BibTex</a>
      <a class="project-link" href=https://drive.google.com/file/d/1QkEnqApB7U7XvRBupfnysgdFMarhnwlP/view?usp=sharing>Video</a>
      <a class="project-link" href=https://docs.google.com/presentation/d/1okTa2eWg-BbjA7l1dHL8R_UWOmF5wSQ0HHm7A8NpvfM/edit?usp=sharing>Slides</a>
      </div></div><br>
      Debidatta Dwibedi, Jonathan Tompson, Corey Lynch, Pierre Sermanet @ IROS 2018
      <br><br>
      We learn continuous control entirely from raw pixels.<br>
      We use a multi-frame TCN to self-supervise task-agnostic representations from vision only, using 2 slightly different views of the cheetah.<br>
      Then using RL on top of our embeddings we learn the cheetah task almost as well as if we were using the true proprioceptive states of the cheetah.
    </td>
  </tr>
  <!-- project block -->
  <tr>
    <td class="project-fig"><div class="figure">
      <video class="b-lazy" data-src="assets/tcn/kuka_pouring_cropped960x540.mp4"
        type="video/mp4" autoplay muted playsinline loop style="display: block; width: 100%;"></video>
        <br>
      <video class="b-lazy" data-src="assets/tcn/pose_all_cropped960x540.mp4"
        type="video/mp4" autoplay muted playsinline loop style="display: block; width: 100%;"></video>
    </div></td>
    <td class="project-cell">
      <div class="project-title" id="tcn">Time-Contrastive Networks (TCN)</div>
      <dt-byline><div class="byline">
      <a class="project-link" href=https://sermanet.github.io/imitate>Project Page</a>
      <a class="project-link" href=https://arxiv.org/abs/1704.06888>Paper</a>
      <a class="project-link" href=https://github.com/sermanet/sermanet.github.io/blob/master/assets/bib/Sermanet2017TCN.bib>BibTex</a>
      <a class="project-link" href=https://www.youtube.com/watch?v=b1UTUQpxPSY>Video</a>
      <a class="project-link" href=https://sites.google.com/site/brainrobotdata/home/multiview-pouring>Dataset</a>
      <a class="project-link" href=https://github.com/tensorflow/models/tree/master/research/tcn>Code</a>
      <a class="project-link" href=https://docs.google.com/presentation/d/1EvWSbsFfnceBpN7yG1wnqM2LxySQ0Gi-wTx6QaoVekY/edit?usp=sharing>Slides</a>
      <a class="project-link" href=https://ai.googleblog.com/2017/07/teaching-robots-to-understand-semantic.html>Blogpost</a>
      </div></div><br>
      Pierre Sermanet, Corey Lynch, Yevgen Chebotar, Jasmine Hsu, Eric Jang, Stefan Schaal, Sergey Levine @ ICRA 2018
      <br><br>
      We propose a general self-supervised method for learning representations from raw unlabeled videos.<br>
      We show that the self-supervised representations are rich enough to perform robotic tasks.<br>
      <br>
      We use the distance in our learned embedding space to a video demonstration as a reward. An RL algorithm can learn to perform a pouring task using this reward.
      The robot has learned to pour in only 9 iterations using a single video demonstration, while never receiving any labels.<br>
      <br>
      We also show that a robot can teach itself how to imitate people: by training a single TCN on videos of both humans and robots peforming random motions, the TCN model is able to find correspondences between humans and robots, despite never being given any label correspondences.
    </td>
  </tr>
  <!-- project block -->
  <tr>
    <td class="project-fig"><div class="figure">
      <video class="b-lazy" data-src="assets/rewards/doors.mp4"
        type="video/mp4" autoplay muted playsinline loop style="display: block; width: 100%;"></video>
        <br>
      <img class='project-img' src='assets/rewards/rewards.png'>
    </div></td>
    <td class="project-cell">
      <div class="project-title" id="perceptual">Unsupervised Perceptual Rewards</div>
      <dt-byline><div class="byline">
      <a class="project-link" href=https://sermanet.github.io/rewards>Project Page</a>
      <a class="project-link" href=https://arxiv.org/abs/1612.06699>Paper</a>
      <a class="project-link" href=https://github.com/sermanet/sermanet.github.io/blob/master/assets/bib/Sermanet2017Rewards.bib>BibTex</a>
      <a class="project-link" href=https://youtu.be/7f7sdLMCItg>Video</a>
      <a class="project-link" href=https://sites.google.com/site/brainrobotdata/home/pouring-dataset>Dataset</a>
      <a class="project-link" href=https://ai.googleblog.com/2017/07/teaching-robots-to-understand-semantic.html>Blogpost</a>
      </div></div><br>
      Pierre Sermanet, Kelvin Xu, Sergey Levine @ RSS 2017
      <br><br>
      We propose learning unsupervised perceptual rewards that can be fed to an RL system and show it is able to learn a robotic task such as door opening from a few human demonstrations.
    </td>
  </tr>
  <!-- project block -->
  <tr>
    <td class="project-fig"><div class="figure"><video class="b-lazy" data-src="assets/misc/attention.mp4"
        type="video/mp4" autoplay muted playsinline loop style="display: block; width: 100%;"></video></div></td>
    <td class="project-cell">
      <div class="project-title">Visual Attention</div>
      <dt-byline><div class="byline">
      <a class="project-link" href=https://arxiv.org/abs/1412.7054>Paper</a>
      <a class="project-link" href=https://github.com/sermanet/sermanet.github.io/blob/master/assets/bib/Sermanet2015Attention.bib>BibTex</a>
      </div></div><br>
      Pierre Sermanet, Andrea Frome, Esteban Real @ ICLR 2015 (workshop)
      <br><br>
      We demonstrate a foveated attention RNN that is able to perform fine-grained classification.<br>
      Tracking naturally emerges from our fovated model when ran on videos, even though it was only trained on still images.
    </td>
  </tr>
  <!-- project block -->
  <tr>
    <td class="project-fig"><div class="figure">
      <img class='project-img' src='assets/inception/googlenet_diagram.png'>
      <img class='project-img' src='assets/inception/ILSVRC2012_val_00021791.PNG'>
    </div></td>
    <td class="project-cell">
      <div class="project-title">Inception / GoogLeNet</div>
      <dt-byline><div class="byline">
      <a class="project-link" href=https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Szegedy_Going_Deeper_With_2015_CVPR_paper.pdf>Paper</a>
      <a class="project-link" href=https://github.com/sermanet/sermanet.github.io/blob/master/assets/bib/Szegedy2015Inception.bib>BibTex</a>
      <a class="project-link" href=https://github.com/tensorflow/models/tree/master/research/inception>Code</a>
      <a class="project-link" href=http://www.image-net.org/challenges/LSVRC/2014/results>ImageNet Results</a>
      <a class="project-link" href=https://ai.googleblog.com/2014/09/building-deeper-understanding-of-images.html>Blogpost</a>
      </div></div><br>
      Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, Andrew Rabinovich @ CVPR 2015
      <br><br>
      A deep architecture for computer vision. Our model obtained 1st place for the classification and detection tasks in the 2014 ImageNet Challenge.
    </td>
  </tr>
  <!-- project block -->
  <tr>
    <td class="project-fig"><div class="figure">
      <img class='project-img' src='assets/nyu/cat_dogs.png'>
    </div></td>
    <td class="project-cell">
      <div class="project-title">Dogs vs. Cats Kaggle challenge</div>
      <dt-byline><div class="byline">
      <a class="project-link" href=https://www.kaggle.com/c/dogs-vs-cats/leaderboard>Leaderboard</a>
      </div></div><br>
      Pierre Sermanet (2014)
      <br><br>
      1st place in an image classification Kaggle challenge between dog and cat images. Most of the top entries are based on our OverFeat model.
    </td>
  </tr>
  <!-- project block -->
  <tr>
    <td class="project-fig"><div class="figure">
      <img class='project-img' src='assets/overfeat/ms_nofine_clean.png'>
      <img class='project-img' src='assets/overfeat/bear.png'>
    </div></td>
    <td class="project-cell">
      <div class="project-title" id="overfeat">OverFeat</div>
      <dt-byline><div class="byline">
      <a class="project-link" href=https://github.com/sermanet/OverFeat>Code</a>
      <a class="project-link" href=https://arxiv.org/abs/1312.6229>Paper</a>
      <a class="project-link" href=https://github.com/sermanet/sermanet.github.io/blob/master/assets/bib/Sermanet2014OverFeat.bib>BibTex</a>
      <a class="project-link" href=http://www.image-net.org/challenges/LSVRC/2013/results.php>ImageNet Results</a>
      <a class="project-link" href=http://www.image-net.org/challenges/LSVRC/2013/slides/overfeat_ilsvrc2013.pdf>Slides</a>
      <a class="project-link" href=https://machinelearning.apple.com/2017/11/16/face-detection.html>Press</a>
      </div></div><br>
      Pierre Sermanet, David Eigen, Xiang Zhang, Michael Mathieu, Rob Fergus, Yann LeCun @ ICLR 2014
      <br><br>
      This model obtained 1st place in the 2013 ImageNet object localization challenge. The model and pre-trained features were later released to the public.
      <br><br>
      Overfeat has been used by Apple for on-device face detection in iPhones: <a class="project-link" href=https://machinelearning.apple.com/2017/11/16/face-detection.html>blogpost</a>
    </td>
  </tr>
  <!-- project block -->
  <tr>
    <td class="project-fig"><div class="figure"><video class="b-lazy" data-src="assets/pedestrians/pedestrians_broadway480x360.mp4"
        type="video/mp4" autoplay muted playsinline loop style="display: block; width: 100%;"></video></div></td>
    <td class="project-cell">
      <div class="project-title">Pedestrian Detection</div>
      <dt-byline><div class="byline">
      <a class="project-link" href=https://www.cv-foundation.org/openaccess/content_cvpr_2013/html/Sermanet_Pedestrian_Detection_with_2013_CVPR_paper.html>Paper</a>
      <a class="project-link" href=https://github.com/sermanet/sermanet.github.io/blob/master/assets/bib/Sermanet2013Pedestrian.bib>BibTex</a>
      <a class="project-link" href=https://www.youtube.com/watch?v=uKU2pzpGUlM>Video</a>
      </div></div><br>
      Pierre Sermanet, Koray Kavukcuoglu, Soumith Chintala, Yann Lecun @ CVPR 2013
      <br><br>
      State of the art results on pedestrian detection datasets using deep ConvNets in the EBLearn framework.
    </td>
  </tr>
  <!-- project block -->
  <tr>
    <td class="project-fig"><div class="figure">
      <img class='project-img' src='assets/misc/house.png'>
    </div></td>
    <td class="project-cell">
      <div class="project-title">Convolutional Neural Networks Applied to House Numbers Digit Classification</div>
      <dt-byline><div class="byline">
      <a class="project-link" href=https://arxiv.org/pdf/1204.3968.pdf>Paper</a>
      <a class="project-link" href=https://github.com/sermanet/sermanet.github.io/blob/master/assets/bib/Sermanet2012Convolutional.bib>BibTex</a>
      </div></div><br>
      Pierre Sermanet, Soumith Chintala, Yann LeCun @ ICPR 2012
      <br><br>
      State of the art results in house numbers classification using deep ConvNets.
    </td>
  </tr>
  <!-- project block -->
  <tr>
    <td class="project-fig"><div class="figure">
      <img class='project-img' src="assets/misc/traffic_signs.png"><br>
      <img class='project-img' src='assets/misc/convnet_skip_connections.png'>
    </div></td>
    <td class="project-cell">
      <div class="project-title">Traffic Sign Recognition</div>
      <dt-byline><div class="byline">
      <a class="project-link" href=papers/sermanet-ijcnn-11.pdf>Paper</a>
      <a class="project-link" href=https://github.com/sermanet/sermanet.github.io/blob/master/assets/bib/Sermanet2011Traffic.bib>BibTex</a>
      </div></div><br>
      Pierre Sermanet, Yann LeCun @ IJCNN 2011
      <br><br>
      This deep model obtained 2nd place in a traffic sign recognition challenge using the EBLearn framework. It uses skip connections in deep ConvNets to better combine low-lvel and high-level learned features.
    </td>
  </tr>
  <!-- project block -->
  <tr>
    <td class="project-fig"><div class="figure">
      <img class='project-img' src='assets/misc/unsupervised_filters.png'>
    </div></td>
    <td class="project-cell">
      <div class="project-title">Unsupervised Convolutional Feature Hierarchies</div>
      <dt-byline><div class="byline">
      <a class="project-link" href=http://papers.nips.cc/paper/4133-learning-convolutional-feature-hierarchies-for-visual-recognition.pdf>Paper</a>
      <a class="project-link" href=https://github.com/sermanet/sermanet.github.io/blob/master/assets/bib/Kavukcuoglu2010Unsupervised.bib>BibTex</a>
      </div></div><br>
      Koray Kavukcuoglu, Pierre Sermanet, Y-Lan Boureau, Karol Gregor, Michael Mathieu, Yann LeCun @ NIPS 2010
      <br><br>
      An unsupervised method for learning multi-stage hierarchies of sparse convolutional features. One of the few instances of this period where unsupervised pretraining improved results in a supervised task.
    </td>
  </tr>
  <!-- project block -->
  <tr>
    <td class="project-fig"><div class="figure">
      <img class='project-img' src='assets/nyu/faces.png'>
      <img class='project-img' src='assets/nyu/eblearn.png'>
    </div></td>
    <td class="project-cell">
      <div class="project-title" id="eblearn">EBLearn</div>
      <dt-byline><div class="byline">
      <a class="project-link" href=http://eblearn.sourceforge.net/>Code</a>
      <a class="project-link" href=papers/sermanet-ictai-09.pdf>Paper</a>
      <a class="project-link" href=https://github.com/sermanet/sermanet.github.io/blob/master/assets/bib/Sermanet2009EBLearn.bib>BibTex</a>
      </div></div><br>
      Pierre Sermanet, Koray Kavukcuoglu, Yann LeCun @ ICTAI 2009<br>
      Additional help from Soumith Chintala.
      <br><br>
      A C++ deep learning framework similar to Torch and used for multiple state of the art results in computer vision.
    </td>
  </tr>
  <!-- project block -->
  <tr>
    <td class="project-fig"><div class="figure">
      <video class="b-lazy" data-src="assets/nyu/rovio.mp4"
        type="video/mp4" autoplay muted playsinline loop style="display: block; width: 100%;"></video>
      <video class="b-lazy" data-src="assets/nyu/3pi.mp4"
        type="video/mp4" autoplay muted playsinline loop style="display: block; width: 100%;"></video>
    </div></td>
    <td class="project-cell">
      <div class="project-title">Teaching Assitant for NYU Robotics class</div>
      <dt-byline><div class="byline">
      <a class="project-link" href=https://cs.nyu.edu/~yann/2009s-V22-0480-001/index.html>Class Page</a>
      </div></div><br>
      Pierre Sermanet, Yann LeCun (2009)<br><br>
    </td>
  </tr>
  <!-- project block -->
  <tr>
    <td class="project-fig"><div class="figure"><video class="b-lazy" data-src="assets/lagr/lagr_run_490x360.mp4"
        type="video/mp4" autoplay muted playsinline loop style="display: block; width: 100%;"></video></div></td>
    </div></td>
    <td class="project-cell">
      <div class="project-title">LAGR: Learning Applied to Ground Robots</div>
      <dt-byline><div class="byline">
      <a class="project-link" href=https://cs.nyu.edu/~yann/research/lagr/index.html>Project Page</a>
      <a class="project-link" href=https://www.youtube.com/watch?v=lowcgokiRG8>Video</a>
      </div></div><br>
      Yann LeCun, Urs Muller, Pierre Sermanet, Marco Scoffier, Chris Crudelle, Beat Flepp, Ayse Erkan, Matt Grimes, Raia Hadsell, Koray Kavakcuoglu, Marc'Aurelio Ranzato, Jan Ben, Sumit Chopra, Jeff Han, Marc Peyote, Marc'Aurelio Ranzato, Ilya Rosenberg, Yury Sulsky<br>
      <br>
      A DARPA challenge where the NYU-NetScale team developed ConvNets for long-range off-road navigation from 2004 to 2008.
    </td>
  </tr>
  <!-- project block -->
  <tr>
    <td class="project-fig"><div class="figure"><video class="b-lazy" data-src="assets/lagr/lagr_naviguation480x360.mp4"
        type="video/mp4" autoplay muted playsinline loop style="display: block; width: 100%;"></video></div></td>
    <td class="project-cell">
      <div class="project-title">Learning Long-Range Vision for Autonomous Off-Road Driving</div>
      <dt-byline><div class="byline">
      <a class="project-link" href=papers/hadsell-jfr-09.pdf>Paper</a>
      <a class="project-link" href=https://github.com/sermanet/sermanet.github.io/blob/master/assets/bib/Hadsell2009Learning.bib>BibTex</a>
      </div></div><br>
      Raia Hadsell, Pierre Sermanet, Jan Ben, Ayse Erkan, Marco Scoffier, Koray Kavukcuoglu, Urs Muller, Yann LeCun @ JFR 2009
      <br><br>
      An overview paper of our self-supervised deep learning vision model.
    </td>
  </tr>
  <!-- project block -->
  <tr>
    <td class="project-fig"><div class="figure"><video class="b-lazy" data-src="assets/lagr/lagr_dynamics3_490x360.mp4"
        type="video/mp4" autoplay muted playsinline loop style="display: block; width: 100%;"></video></div></td>
    <td class="project-cell">
      <div class="project-title" id="fast-slow">Collision-Free Off-Road Robot Navigation</div>
      <dt-byline><div class="byline">
      <a class="project-link" href=papers/sermanet-jfr-09.pdf>Paper</a>
      <a class="project-link" href=https://github.com/sermanet/sermanet.github.io/blob/master/assets/bib/Sermanet2009Multirange.bib>BibTex</a>
      </div></div><br>
      Pierre Sermanet, Raia Hadsell, Marco Scoffier, Matt Grimes, Jan Ben, Ayse Erkan, Chris Crudele, Urs Muller, Yann LeCun @ JFR 2009
      <br><br>
      An overview paper of our navigation system designed to naturally handle errors and outputs coming out of a deep vision model. This model decouples the fast and short-range navigation from the slow and long-range navigation to achieve robustness. 
    </td>
  </tr>
  <!-- project block -->
  <tr>
    <td class="project-fig"><div class="figure">
      <img src='assets/lagr/maneuvers.png' width='300'>
      <video class="b-lazy" data-src="assets/lagr/lagr_dynamics480x360.mp4"
        type="video/mp4" autoplay muted playsinline loop style="display: block; width: 100%;"></video>
    </div></td>
    <td class="project-cell">
      <div class="project-title" id="maneuvers">Learning Maneuver Dictionaries for Ground Robot Planning</div>
      <dt-byline><div class="byline">
      <a class="project-link" href=papers/sermanet-isr-08.pdf>Paper</a>
      <a class="project-link" href=https://github.com/sermanet/sermanet.github.io/blob/master/assets/bib/Sermanet2008Learning.bib>BibTex</a>
      <a class="project-link" href=https://www.youtube.com/watch?v=kCF7Uo5vwJY>Video</a>
      </div></div><br>
      Pierre Sermanet, Marco Scoffier, Chris Crudele, Urs Muller, Yann LeCun @ ISR 2008
      <br><br>
      Instead of computing the theoretical dynamics of a vehicle, we propose to simply record the observed dynamics while a human operator "plays" with the robot, essentially trying all possible moves. At test time, the model has a bank of observed possible trajectories for every state of the motors. Trajectories leading to collisions are discarded, while the fastest available trajectory is selected. While we observed many collisions using the baseline system, we did not observe collisions after introducing this model.
    </td>
  </tr>
  <!-- project block -->
  <tr>
    <td class="project-fig"><div class="figure">
      <video class="b-lazy" data-src="assets/lagr/lagr_mapping480x360.mp4"
        type="video/mp4" autoplay muted playsinline loop style="display: block; width: 100%;"></video>
      <video class="b-lazy" data-src="assets/lagr/lagr_mapping2_hyperbolic_540x320.mp4"
        type="video/mp4" autoplay muted playsinline loop style="display: block; width: 100%;"></video>
    </div></td>
    <td class="project-cell">
      <div class="project-title">Mapping and Planning under Uncertainty in Mobile Robots with Long-Range Perception</div>
      <dt-byline><div class="byline">
      <a class="project-link" href=papers/sermanet-iros-08.pdf>Paper</a>
      <a class="project-link" href=https://github.com/sermanet/sermanet.github.io/blob/master/assets/bib/Sermanet2008Mapping.bib>BibTex</a>
      <a class="project-link" href=https://www.youtube.com/watch?v=ByToYYTOhmk>Video</a>
      </div></div><br>
      Pierre Sermanet, Raia Hadsell, Marco Scoffier, Urs Muller, Yann LeCun @ IROS 2008
      <br><br>
      A hyperbolic-polar coordinate mapping system that is naturally suited to handle imprecisions in long-range visual navigation.
    </td>
  </tr>
  <!-- project block -->
  <tr>
    <td class="project-fig"><div class="figure">
      <img class='project-img' src='assets/lagr/long_range.png'>
    </div></td>
    <td class="project-cell">
      <div class="project-title" id="lagr-deep-learning">Deep Belief Net Learning in a Long-Range Vision System</div>
      <dt-byline><div class="byline">
      <a class="project-link" href=papers/hadsell-iros-08.pdf>Paper</a>
      <a class="project-link" href=https://github.com/sermanet/sermanet.github.io/blob/master/assets/bib/Hadsell2008Deep.bib>BibTex</a>
      </div></div><br>
      Raia Hadsell, Ayse Erkan, Pierre Sermanet, Marco Scoffier, Urs Muller, Yann LeCun @ IROS 2008
      <br><br>
      Self-supervised long-range visual navigation with deep ConvNets.
    </td>
  </tr>
  <!-- project block -->
  <tr>
    <td class="project-fig"><div class="figure">
      <img class='project-img' src='assets/lagr/online_learning.png'>
    </div></td>
    <td class="project-cell">
      <div class="project-title">Online Learning for Offroad Robots</div>
      <dt-byline><div class="byline">
      <a class="project-link" href=papers/hadsell-rss-07.pdf>Paper</a>
      <a class="project-link" href=https://github.com/sermanet/sermanet.github.io/blob/master/assets/bib/Hadsell2007Online.bib>BibTex</a>
      </div></div><br>
      Raia Hadsell, Pierre Sermanet, Ayse Naz Erkan, Jan Ben, Jefferson Han, Beat Flepp, Urs Muller, Yann LeCun @ RSS 2007
      <br><br>
      Online adaptation of long-range vision by self-supervising with short-range stereo vision.
    </td>
  </tr>
  <!-- project block -->
  <tr>
    <td class="project-fig"><div class="figure"><video class="b-lazy" data-src="assets/eurobot/robot_hangs_out2.mp4"
        type="video/mp4" autoplay muted playsinline loop style="display: block; width: 100%;"></video></div></td>
    <td class="project-cell">
      <div class="project-title">EUROBOT 2004 Competition</div>
      <dt-byline><div class="byline">
      <a class="project-link" href=https://sermanet.github.io/eurobot>Project Page</a>
      </div></div><br>
      Computer vision, navigation and behaviors by Pierre Sermanet, Philippe Rambert, Jean-Baptiste Mouret<br>
      Entire team: <a class="link" href=http://www.evolutek.org/>Evolutek</a>
      <br><br>
      Vision-based behaviors in a robot-rugby challenge.
    </td>
  </tr>
  <!-- <\!-- project block -\-> -->
  <!-- <tr> -->
  <!--   <td class="project-fig"><div class="figure"><video class="b-lazy" data-src="assets/" -->
  <!--       type="video/mp4" autoplay muted playsinline loop style="display: block; width: 100%;"></video></div></td> -->
  <!--   <td class="project-cell"> -->
  <!--     <div class="project-title"></div> -->
  <!--     <dt-byline><div class="byline"> -->
  <!--     <a class="project-link" href=>Project Page</a> -->
  <!--     <a class="project-link" href=>Paper</a> -->
  <!--     <a class="project-link" href=https://github.com/sermanet/sermanet.github.io/blob/master/assets/bib/>BibTex</a> -->
  <!--     <a class="project-link" href=>Video</a> -->
  <!--     <a class="project-link" href=>Slides</a> -->
  <!--     </div></div><br> -->
  <!--     <br><br> -->
  <!--     Description. -->
  <!--   </td> -->
  <!-- </tr> -->
</table>
</div>
</dt-byline>
</dt-article>
<dt-appendix>
<h2>Acknowledgments</h2>
<p>This article was prepared using the <a href="https://distill.pub">Distill</a> <a href="https://github.com/distillpub/template">template</a>.</p>
</dt-appendix>
</dt-appendix>
</body>
<script type="text/bibliography">
</script>

<script src="lib/blazy.js"></script>
<script src="lib/video-optimizer.js"></script>
<script>
  // Initialize Blazy for images (non-video elements)
  var bLazy = new Blazy({
    selector: 'img.b-lazy, .b-lazy:not(video)', // Exclude videos from basic lazy loading
    success: function(){
      updateCounter();
    }
  });
  
  var imageLoaded = 0;
  function updateCounter() {
    imageLoaded++;
    console.log("blazy image loaded: "+imageLoaded);
  }
  
  // Video optimization settings
  var videoOptimizer = new VideoOptimizer({
    rootMargin: '50px', // Start loading videos 50px before they come into view
    enablePlaceholder: false, // Disable placeholders to show videos immediately  
    preferredFormats: ['webm', 'mp4'], // Try WebM first for better compression
    fadeInDuration: 200,
    retryAttempts: 2,
    autoplay: true // Ensure autoplay is enabled by default
  });
  
  // Optional: Add performance monitoring
  window.addEventListener('load', function() {
    setTimeout(function() {
      var stats = videoOptimizer.getStats();
      console.log('Video loading stats:', stats);
    }, 2000);
  });
</script>
